{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import sys\n",
    "import json\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import random as rn\n",
    "import keras\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All this for reproducibility\n",
    "np.random.seed(1)\n",
    "rn.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bishn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Summary:min/avg/median/std 85/86/87/88/89/90/95/99/max:\n",
      "3 116.47778 86.0 88.1847205941687 189.0 195.0 203.0 211.0 220.0 230.0 302.0 457.0 1388\n",
      "X, labels #classes classes 50000 (50000,) 2 ['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# Build the corpus and sequences\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk_stopw = stopwords.words('english')\n",
    "sequenceLength = 200\n",
    "\n",
    "\n",
    "\n",
    "def tokenize (text):        #   no punctuation & starts with a letter & between 2-15 characters in length\n",
    "    tokens = [word.strip(string.punctuation) for word in RegexpTokenizer(r'\\b[a-zA-Z][a-zA-Z0-9]{2,14}\\b').tokenize(text)]\n",
    "    return  [f.lower() for f in tokens if f and f.lower() not in nltk_stopw]\n",
    "\n",
    "def getMovies():\n",
    "    X, labels, labelToName  = [], [], { 0 : 'neg', 1: 'pos' }\n",
    "    for dataset in ['train', 'test']:\n",
    "        for classIndex, directory in enumerate(['neg', 'pos']):\n",
    "            dirName = 'E:\\\\IMDB_Dataset\\\\aclImdb\\\\' + dataset + \"\\\\\" + directory\n",
    "            for reviewFile in os.listdir(dirName):\n",
    "                with open (dirName + '\\\\' + reviewFile, 'r', encoding=\"utf8\") as f:\n",
    "                    tokens = tokenize (f.read())\n",
    "                    if (len(tokens) == 0):\n",
    "                        continue\n",
    "                X.append(tokens)\n",
    "                labels.append(classIndex)\n",
    "    nTokens = [len(x) for x in X]\n",
    "    return X, np.array(labels), labelToName, nTokens\n",
    "\n",
    "X, labels, labelToName, nTokens = getMovies()\n",
    "print ('Token Summary:min/avg/median/std 85/86/87/88/89/90/95/99/max:',)\n",
    "print (np.amin(nTokens), np.mean(nTokens),np.median(nTokens),np.std(nTokens),np.percentile(nTokens,85),np.percentile(nTokens,86),np.percentile(nTokens,87),np.percentile(nTokens,88),np.percentile(nTokens,89),np.percentile(nTokens,90),np.percentile(nTokens,95),np.percentile(nTokens,99),np.amax(nTokens))\n",
    "labelToNameSortedByLabel = sorted(labelToName.items(), key=lambda kv: kv[0]) # List of tuples sorted by the label number [ (0, ''), (1, ''), .. ]\n",
    "namesInLabelOrder = [item[1] for item in labelToNameSortedByLabel]\n",
    "numClasses = len(namesInLabelOrder)\n",
    "print ('X, labels #classes classes {} {} {} {}'.format(len(X), str(labels.shape), numClasses, namesInLabelOrder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab padded_docs 98089 (50000, 200)\n"
     ]
    }
   ],
   "source": [
    "kTokenizer = keras.preprocessing.text.Tokenizer()\n",
    "kTokenizer.fit_on_texts(X)\n",
    "encoded_docs = kTokenizer.texts_to_sequences(X)\n",
    "Xencoded = keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=sequenceLength, padding='post')\n",
    "print ('Vocab padded_docs {} {}'.format(len(kTokenizer.word_index), str(Xencoded.shape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test & Train Split\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1).split(Xencoded, labels)\n",
    "train_indices, test_indices = next(sss)\n",
    "train_x, test_x = Xencoded[train_indices], Xencoded[test_indices]\n",
    "train_labels = keras.utils.to_categorical(labels[train_indices], len(labelToName))\n",
    "test_labels = keras.utils.to_categorical(labels[test_indices], len(labelToName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 200, 300)          29427000  \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 197, 150)          180150    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 49, 150)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 7350)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 14702     \n",
      "=================================================================\n",
      "Total params: 29,621,852\n",
      "Trainable params: 29,621,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=2, mode='auto', restore_best_weights=False)\n",
    "model = keras.models.Sequential()\n",
    "embedding = keras.layers.embeddings.Embedding(input_dim=len(kTokenizer.word_index)+1, output_dim=300, input_length=sequenceLength, trainable=True)\n",
    "model.add(embedding)\n",
    "model.add(keras.layers.Conv1D(150, 4, activation='relu', padding='valid'))\n",
    "model.add(keras.layers.MaxPooling1D(4, padding = 'valid'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(numClasses, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 229s - loss: 0.3277 - acc: 0.8557 - val_loss: 0.2617 - val_acc: 0.8969\n",
      "Epoch 2/50\n",
      " - 224s - loss: 0.0913 - acc: 0.9688 - val_loss: 0.4430 - val_acc: 0.8619\n",
      "Epoch 3/50\n",
      " - 223s - loss: 0.0130 - acc: 0.9967 - val_loss: 0.6000 - val_acc: 0.8703\n",
      "Epoch 4/50\n",
      " - 224s - loss: 0.0085 - acc: 0.9976 - val_loss: 0.9850 - val_acc: 0.8517\n",
      "Epoch 5/50\n",
      " - 224s - loss: 0.0214 - acc: 0.9931 - val_loss: 0.8479 - val_acc: 0.8713\n",
      "Epoch 6/50\n",
      " - 224s - loss: 0.0145 - acc: 0.9959 - val_loss: 0.9176 - val_acc: 0.8708\n",
      "Epoch 00006: early stopping\n",
      "10000/10000 [==============================] - 2s 240us/step\n",
      "[[4275  725]\n",
      " [ 567 4433]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg     0.8829    0.8550    0.8687      5000\n",
      "         pos     0.8594    0.8866    0.8728      5000\n",
      "\n",
      "   micro avg     0.8708    0.8708    0.8708     10000\n",
      "   macro avg     0.8712    0.8708    0.8708     10000\n",
      "weighted avg     0.8712    0.8708    0.8708     10000\n",
      "\n",
      "Time Taken: 1352.1559059619904\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "result = {}\n",
    "history = model.fit(x=train_x, y=train_labels, epochs=50, batch_size=32, shuffle=True, validation_data = (test_x, test_labels), verbose=2, callbacks=[early_stop])\n",
    "result['history'] = history.history\n",
    "result['test_loss'], result['test_accuracy'] = model.evaluate(test_x, test_labels, verbose=1)\n",
    "predicted = model.predict(test_x, verbose=2)\n",
    "predicted_labels = predicted.argmax(axis=1)\n",
    "result['confusion_matrix'] = confusion_matrix(labels[test_indices], predicted_labels).tolist()\n",
    "result['classification_report'] = classification_report(labels[test_indices], predicted_labels, digits=4, target_names=namesInLabelOrder, output_dict=True)\n",
    "print (confusion_matrix(labels[test_indices], predicted_labels))\n",
    "print (classification_report(labels[test_indices], predicted_labels, digits=4, target_names=namesInLabelOrder))\n",
    "elapsed_time = time.time() - start_time\n",
    "print ('Time Taken:', elapsed_time)\n",
    "result['elapsed_time'] = elapsed_time\n",
    "\n",
    "f = open ('cnn.json','w')\n",
    "out = json.dumps(result, ensure_ascii=True)\n",
    "f.write(out)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
